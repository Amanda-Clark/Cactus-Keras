{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "80cd9453dfdb10a8ce18d99fc32debc6add8b5d2"
   },
   "source": [
    "I used https://www.kaggle.com/masonblier/aerial-cactus-simple-cnn, https://towardsdatascience.com/image-classification-python-keras-tutorial-kaggle-challenge-45a6332a58b8, and https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py to help get me started. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import imageio\n",
    "from random import shuffle\n",
    "import os\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers. normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def label_img(name):\n",
    "    word_label = name.split('-')[1]\n",
    "    if word_label == 'Cactus': return 1\n",
    "    elif word_label == 'NoCactus' : return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "dffa887b8f8cd06ed9a094f3168834e950d2851f"
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_training_data():\n",
    "    train_load = []\n",
    "    \n",
    "    for img in os.listdir('../input/labeled-training-images/labeled_train/labeled_train'):\n",
    "        strippedName = img.split('.')[0]\n",
    "        label = label_img(strippedName)\n",
    "        path = os.path.join('../input/labeled-training-images/labeled_train/labeled_train', img)\n",
    "        if \"DS_Store\" not in path:\n",
    "            img = Image.open(path)\n",
    "            train_load.append([np.array(img), label])\n",
    "           \n",
    "            \n",
    "    shuffle(train_load)\n",
    "    return train_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "de99384c5f35ab6cd9f8265986adec668f1a6a3c"
   },
   "outputs": [],
   "source": [
    "train_data = load_training_data()\n",
    "\n",
    "trainImage = np.array([i[0] for i in train_data]).reshape(-1, 32, 32, 3)\n",
    "trainLabels = np.array([i[1] for i in train_data])\n",
    "trainLabels = keras.utils.to_categorical(trainLabels, 2)\n",
    "trainImage  = trainImage/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    #rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=.15)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator()#rescale=1. / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-input": false,
    "_uuid": "12e2a85be78b622bf252f013dcfd53b94bd63ac2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(32,32,3)))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_generator = train_datagen.flow_from_directory(\n",
    "#     '../input/dataforaugmentation/labeled_train_augment/labeled_train_augment',\n",
    "#     target_size=(32, 32),\n",
    "#     batch_size=16,\n",
    "#     class_mode='binary')\n",
    "train_datagen.fit(trainImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "8ad85df182a3dd7c0845835a59224ddd35725225"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "4e94cbc3606ab837d3d0eedc348d1f01b9d296c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/75\n",
      "547/546 [==============================] - 18s 32ms/step - loss: 0.2338 - acc: 0.9081\n",
      "Epoch 2/75\n",
      "547/546 [==============================] - 12s 21ms/step - loss: 0.1216 - acc: 0.9531\n",
      "Epoch 3/75\n",
      "547/546 [==============================] - 12s 21ms/step - loss: 0.0844 - acc: 0.9695\n",
      "Epoch 4/75\n",
      "547/546 [==============================] - 12s 21ms/step - loss: 0.0647 - acc: 0.9766\n",
      "Epoch 5/75\n",
      "547/546 [==============================] - 12s 21ms/step - loss: 0.0613 - acc: 0.9777\n",
      "Epoch 6/75\n",
      "547/546 [==============================] - 12s 21ms/step - loss: 0.0615 - acc: 0.9767\n",
      "Epoch 7/75\n",
      "547/546 [==============================] - 12s 21ms/step - loss: 0.0534 - acc: 0.9811\n",
      "Epoch 8/75\n",
      "547/546 [==============================] - 12s 21ms/step - loss: 0.0495 - acc: 0.9822\n",
      "Epoch 9/75\n",
      "547/546 [==============================] - 12s 21ms/step - loss: 0.0545 - acc: 0.9813\n",
      "Epoch 10/75\n",
      "547/546 [==============================] - 12s 21ms/step - loss: 0.0480 - acc: 0.9832\n",
      "Epoch 11/75\n",
      "547/546 [==============================] - 12s 22ms/step - loss: 0.0419 - acc: 0.9854\n",
      "Epoch 12/75\n",
      "547/546 [==============================] - 12s 21ms/step - loss: 0.0523 - acc: 0.9817\n",
      "Epoch 13/75\n",
      "547/546 [==============================] - 11s 21ms/step - loss: 0.0427 - acc: 0.9859\n",
      "Epoch 14/75\n",
      "547/546 [==============================] - 12s 22ms/step - loss: 0.0422 - acc: 0.9839\n",
      "Epoch 15/75\n",
      "547/546 [==============================] - 12s 22ms/step - loss: 0.0403 - acc: 0.9852\n",
      "Epoch 16/75\n",
      "547/546 [==============================] - 12s 21ms/step - loss: 0.0369 - acc: 0.9874\n",
      "Epoch 17/75\n",
      "547/546 [==============================] - 12s 21ms/step - loss: 0.0414 - acc: 0.9851\n",
      "Epoch 18/75\n",
      "547/546 [==============================] - 12s 21ms/step - loss: 0.0444 - acc: 0.9852\n",
      "Epoch 19/75\n",
      "547/546 [==============================] - 12s 22ms/step - loss: 0.0425 - acc: 0.9863\n",
      "Epoch 20/75\n",
      "547/546 [==============================] - 12s 21ms/step - loss: 0.0353 - acc: 0.9877\n",
      "Epoch 21/75\n",
      "547/546 [==============================] - 12s 21ms/step - loss: 0.0400 - acc: 0.9862\n",
      "Epoch 22/75\n",
      "547/546 [==============================] - 12s 21ms/step - loss: 0.0338 - acc: 0.9891\n",
      "Epoch 23/75\n",
      "547/546 [==============================] - 12s 21ms/step - loss: 0.0360 - acc: 0.9876\n",
      "Epoch 24/75\n",
      "547/546 [==============================] - 12s 21ms/step - loss: 0.0345 - acc: 0.9875\n",
      "Epoch 25/75\n",
      "547/546 [==============================] - 12s 21ms/step - loss: 0.0318 - acc: 0.9886\n",
      "Epoch 26/75\n",
      "547/546 [==============================] - 12s 21ms/step - loss: 0.0259 - acc: 0.9913\n",
      "Epoch 27/75\n",
      "547/546 [==============================] - 12s 21ms/step - loss: 0.0296 - acc: 0.9897\n",
      "Epoch 28/75\n",
      "547/546 [==============================] - 12s 22ms/step - loss: 0.0294 - acc: 0.9899\n",
      "Epoch 29/75\n",
      "547/546 [==============================] - 12s 22ms/step - loss: 0.0348 - acc: 0.9874\n",
      "Epoch 30/75\n",
      "547/546 [==============================] - 12s 21ms/step - loss: 0.0278 - acc: 0.9903\n",
      "Epoch 31/75\n",
      "547/546 [==============================] - 12s 21ms/step - loss: 0.0315 - acc: 0.9890\n",
      "Epoch 32/75\n",
      "547/546 [==============================] - 12s 21ms/step - loss: 0.0260 - acc: 0.9907\n",
      "Epoch 33/75\n",
      "547/546 [==============================] - 12s 21ms/step - loss: 0.0280 - acc: 0.9901\n",
      "Epoch 34/75\n",
      "547/546 [==============================] - 12s 21ms/step - loss: 0.0324 - acc: 0.9884\n",
      "Epoch 35/75\n",
      "242/546 [============>.................] - ETA: 6s - loss: 0.0257 - acc: 0.9912"
     ]
    }
   ],
   "source": [
    "#model.fit(trainImage, trainLabels, epochs = 15, verbose = 1, validation_split=.15)\n",
    "#model.fit_generator(train_generator, steps_per_epoch=17500//16, verbose=1)\n",
    "model.fit_generator(train_datagen.flow(trainImage, trainLabels, batch_size=32), steps_per_epoch=len(trainImage)/32, epochs=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "5bdfd555eb4e9c2647e263599541127db85163c9"
   },
   "outputs": [],
   "source": [
    "def load_test_data():\n",
    "    test_data = []\n",
    "    names =[]\n",
    "    for img in os.listdir(\"../input/aerial-cactus-identification/test/test\"):\n",
    "        name = img\n",
    "        path = os.path.join(\"../input/aerial-cactus-identification/test/test\", img)\n",
    "        if \"DS_Store\" not in path:\n",
    "            img = Image.open(path)\n",
    "            test_data.append([np.array(img)])\n",
    "            names.append(name)\n",
    "            \n",
    "    \n",
    "    return (test_data, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "914a1db0c208441def6b80162edcf2cd1fcc309b"
   },
   "outputs": [],
   "source": [
    "\n",
    "(test_data, names) = load_test_data()\n",
    "\n",
    "testImages = np.array([i[0] for i in test_data]).reshape(-1, 32, 32, 3)\n",
    "testImages = testImages/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "bd812c9c825de1ef5bb74d74e7e61459e0cc0a88"
   },
   "outputs": [],
   "source": [
    "results = model.predict_classes(testImages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "715881d8a012d4d4a85f0db84046dcba45eefc30"
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame({'id':names,'has_cactus': results})\n",
    "df.index.name='id'\n",
    "\n",
    "df.to_csv(\"keras_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "4c6c090dc00ff62e33dd75601adb51d9716a2879"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
